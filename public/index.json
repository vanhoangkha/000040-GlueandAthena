[
{
	"uri": "/",
	"title": "Cost and Performance Analysis with AWS Glue and Amazon Athena",
	"tags": [],
	"description": "",
	"content": "Cost and performance analysis with AWS Glue and Amazon Athena The lab will guide you step by step to set up a foundation for analyzing the cost and performance reports of any system. The knowledge gained from the lab will help you perform a cost and performance analysis of the system you are working on.\nContent  Introduction Preparation steps Cost and performance analysis Resource Cleanup  "
},
{
	"uri": "/3-costandusageanalysis/3.1-datainthetable/",
	"title": "Data in the Table",
	"tags": [],
	"description": "",
	"content": "\rNote: For the convenience of testing, you can copy the query commands. Replace our data table with your respective table name. For each command, we just need to copy it into the query window and then click the Run button.\n\rData in the Table  We perform the query data 10 records do not overlap located in the table line_item_line_item_description  SELECT distinct \u0026#34;line_item_line_item_description\u0026#34;\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rLIMIT 10;  Select Run  Result after query  Do a query 10 records where the column value line_item_line_item_type is Usage  SELECT * FROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rWHERE \u0026#34;line_item_line_item_type\u0026#34; like \u0026#39;%Usage%\u0026#39;\rLIMIT 10; Query results returned  Show duplicate payment times in the datasheet  SELECT distinct bill_billing_period_start_date\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rLIMIT 10; Query results returned. Monthly payment results.  "
},
{
	"uri": "/1-introduce/",
	"title": "Introducing AWS Glue &amp; Amazon Athena",
	"tags": [],
	"description": "",
	"content": "Introducing AWS Glue \u0026amp; Amazon Athena AWS Glue is a data preparation service that assists data analysts in extracting - transforming - pushing ETL(Extract - Transform - Load) data, facilitating the refining, enriching, and normalizing of data. Also, thanks to AWS Glue, the time required to start analyzing data can be reduced from months to minutes.\nTo make your data ready for analysis, you need to first extract data from multiple sources (e.g. S3). Then it must be refined, transformed into the required format (eg Parquet), and pushed the data into databases, data warehouses, and data lakes for later analysis. These jobs are often performed by multiple teams of people with a variety of tools.\nAWS Glue allows operation both in the form of code and in the form of an interface that makes it convenient for users to prepare data. Data analysts can easily initiate, run, and monitor ETL processes with AWS Glue Studio. There is also AWS Glue DataBrew that refines and normalizes data intuitively without having to write code.\nBelow is a basic operation diagram with a combination of AWS Glue and Amazon Athena\nIn there:\n (1): Data is pushed to AWS S3.  S3 containing input does not require public access configuration and does not need to be in the same region as the AWS Glue service.\n\r (2): AWS Glue uses Glue Crawler to transform the data in S3 into a table (table)  AWS Glue needs GetObject and PutObject S3 permission, to allow Glue to access S3 (read and write Parquet files on S3)\n\r (3): Through the ETL process, AWS Glue transforms tabular data into Parquet (4): The conversion results in step (3) are saved in S3 bucket 2. (5)+(6): Continue using Glue Crawler to convert the Parquet file located in S3 into Parquet Catalog file. (7): Use Amazon Athena to query data from AWS Glue.  S3 contains query results from Athena which must be the same Region as Amazon Athena\n\rHowever, in the scope of the lab, we skip the initial data transformation steps and only work directly on the existing Parquet file. Specifically, we will go to build a platform for cost and performance analysis, from which we can balance the cost with the efficiency of the components in the system, but still adhere to the optimal architecture. Advantages of AWS (AWS Well-Architected Framework)\n"
},
{
	"uri": "/2-prerequiste/2.1-preparedatabase/",
	"title": "Preparing the database",
	"tags": [],
	"description": "",
	"content": "In this lab, we use available data about AWS Cost \u0026amp; Usage Report.\nThe first step, to build a database for query purposes from Amazon Athena through AWS Glue, we need to have data based on AWS Cost \u0026amp; Usage Report. However, getting this report will take at least 24h, so we have prepared the report files.\nYou can download the data at Cost and Usage Analysis. \r Go to AWS Management Console   Find S3 Select S3  In the S3 interface   Select Buckets Select Create bucket   In the Create bucket interface   Bucket name, enter cost-and-usage-analysis  Select Create bucket  Finish creating S3 bucket   Select cost-and-usage-analysis bucket  In cost-and-usage-analysis bucket   Create a folder to store data by selecting Create folder  In the Create folder interface   Folder name, enter Monthly-Report Select Create folder  We perform Upload of data files  Select downloaded data files   Select Add files Select data files  Select Upload  When uploading status changes to Succeeded  "
},
{
	"uri": "/2-prerequiste/2.2-builddatabase/",
	"title": "Building Database",
	"tags": [],
	"description": "",
	"content": "In the lab, once we have the input data with the path on S3 as at the beginning of the lab, we will configure AWS Glue and Crawler so that it runs on a schedule once a day.. Crawler will scan the path containing the input Parquet file, save it on S3 and then create a database with accompanying tables. When a new version of report is available, the data table is automatically updated.\nAmazon Athena helps us access and view parquet file contents through SQL code. Amazon Athena is a serverless solution that supports executing SQL queries on large amounts of data. Athena is charged only for scanned data, unlike a traditional database solution.\nThe detailed configuration steps for Amazon Athena to access data files through AWS Glue are as follows:\n Go to AWS Management Console   Find AWS Glue Select AWS Glue   In the AWS Glue interface   Select Crawlers  In the Crawler interface   Select Add crawler  We perform the crawler configuration steps   In the Crawler info section, enter Crawler name as Cost_MasterCrawler  Crawler source type configuration step   Select Data stores Select Crawl new folders only Select Next  In the Data store interface   Choose a data store, enter S3 Crawl data in, select Specified path in my account Include path, Select path of S3  Specify the file formats that will not be crawled in the exclude patterns section. Including file types .json, .yml, .sql, .csv, .gz, .zip  Select No and select Next  In the IAM Role section   Select Create an IAM role Enter cost_MasterCrawler Select Next  In the Schedule section   Frequency, select Run on demand Select Next  Output configuration step   Select Add database  In the Add database interface   Database name, enter costmaster Select Create  Select Next  Double check and select Finish  Finish creating Crawler   Select Cost_MasterCrawler just created Select Run crawler  Crawler runs for about 5 minutes, results appear as shown with 1 Table added  In the AWS Glue interface   Select Databases Select costmaster database Select View tables  Select monthly_report table  It is easy to see that the record count value is different from 0, which proves that parquet data on S3 has been successfully converted to tabular form by AWS Glue Crawler tool.  "
},
{
	"uri": "/3-costandusageanalysis/3.2-cost/",
	"title": "Cost",
	"tags": [],
	"description": "",
	"content": "For effective optimization, it would be nice to be able to look at the top costs by various categories such as services, descriptions and tags.\n Show Top 10 Most Expensive Account ID  SELECT \u0026#34;line_item_usage_account_id\u0026#34;,\rround(sum(\u0026#34;line_item_unblended_cost\u0026#34;), 2) as cost\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rGROUP BY \u0026#34;line_item_usage_account_id\u0026#34;\rORDER BY cost desc\rLIMIT 10; Query results returned  Show Top10 Most Expensive Services  SELECT \u0026#34;line_item_product_code\u0026#34;,\rround(sum(\u0026#34;line_item_unblended_cost\u0026#34;), 2) as cost\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rGROUP BY \u0026#34;line_item_product_code\u0026#34;\rORDER BY cost desc\rLIMIT 10; Query results returned  Show Top10 Most Expensive Services and Descriptions  SELECT \u0026#34;line_item_product_code\u0026#34;,\r\u0026#34;line_item_line_item_description\u0026#34;,\rround(sum(\u0026#34;line_item_unblended_cost\u0026#34;), 2) as cost\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rWHERE \u0026#34;line_item_product_code\u0026#34; like \u0026#39;%AmazonEC2%\u0026#39;\rGROUP BY \u0026#34;line_item_product_code\u0026#34;,\r\u0026#34;line_item_line_item_description\u0026#34;\rORDER BY cost desc\rLIMIT 10; Query results returned  Show Top 10 Most Expensive On Demand EC2 Uses  SELECT \u0026#34;line_item_product_code\u0026#34;,\r\u0026#34;line_item_line_item_description\u0026#34;,\rround(sum(\u0026#34;line_item_unblended_cost\u0026#34;), 2) as cost\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rWHERE \u0026#34;line_item_product_code\u0026#34; like \u0026#39;%AmazonEC2%\u0026#39;\rand \u0026#34;line_item_usage_type\u0026#34; like \u0026#39;%BoxUsage%\u0026#39;\rGROUP BY \u0026#34;line_item_product_code\u0026#34;,\r\u0026#34;line_item_line_item_description\u0026#34;\rORDER BY cost desc\rLIMIT 10; Query results returned  "
},
{
	"uri": "/2-prerequiste/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "Preparation We skip the initial data transformation steps and only work directly on the existing Parquet file. Specifically, we will go to build a platform for cost and performance analysis, from which we can balance the cost with the efficiency of the components in the system, but still adhere to the optimal architecture. advantage of AWS (AWS Well-Architected Framework). We begin with the following data construction steps:\n Prepare Database Build Database Test Database  "
},
{
	"uri": "/3-costandusageanalysis/",
	"title": "Analysis of cost and usage performance",
	"tags": [],
	"description": "",
	"content": "Cost and performance analysis In this part, we will analyze the system\u0026rsquo;s performance using SQL queries. Use of Amazon Athena will be charged when you query, based on the amount of scanned data ( scan ) which is monthly recorded data and the file is in parquet format. This data has been compressed and fragmented to reduce costs when using Amazon Athena. However, the query should still limit the number of records returned with the limit keyword at the end of the statement.\nSome basic SQL statements   SQL SELECT statement: SELECT is used to select data from the database. The returned data is stored in a result table.\n  SQL SELECT DISTINCT statement: SELECT DISTINCT is used to return distinct (different) values. Inside a table, a column often contains many duplicate values; and sometimes you just want to list different (separate) values.\n  WHERE clause in SQL: WHERE clause is used to filter records. It is only used to extract those records that meet a specific condition.\n  SQL statement GROUP BY: GROUP BY is used to group rows with the same value into summary rows.\n  SQL statement ORDER BY: ORDER BY is used to sort the result set in ascending or descending order.\n  SQL LIKE operator: The LIKE operator is used in a WHERE clause to search for a specific pattern in a column.\n  LIMIT keyword: limit the number of records returned\n  Content  Study the data in the table Cost Study Tagging and Cost Allocation Savings Plans, Reserved Instance, On-Demand and Spot Usage  "
},
{
	"uri": "/2-prerequiste/2.3-testdatabase/",
	"title": "Database Check",
	"tags": [],
	"description": "",
	"content": "We create S3 bucket to store query results.\n Access to S3   Select Bucket Select Create bucket  In the Create bucket interface   Bucket name, enter cost-and-usage-query-results  Select Create bucket  Select cost-and-usage-query-results the newly created bucket  Create a folder to store query data   Select Create folder  Create a folder   Folder name, enter Athena-Query-Results Select Create folder  Finish creating the folder  Access AWS Console Management   Find Athena Select Athena  To manipulate the data table, we need to configure where to save the query results on S3.   Select Settings  Select Manage  Choose the S3 path to store  Then select Edit  Check and execute the query   Data Source, select AwsDataCatalog Database, select costmaster Table, select monthly_report Execute the query with the following command: the table data is saved to Amazon Athena\u0026rsquo;s metastore.  MSCK REPAIR TABLE monthly_report;  Select Run Complete the query Query successful  Perform a system preview to test the ability to query data tables on AWS Glue   Select monthly_report table Select Preview Table  SELECT * FROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rLIMIT 10; "
},
{
	"uri": "/3-costandusageanalysis/3.3-taggingandcost/",
	"title": "Tagging and Cost Allocation",
	"tags": [],
	"description": "",
	"content": "Tagging is quite common in large organizations, it fulfills the requirement of allocating costs back to specific business units. It is also important for optimizing workload allocation and for measuring performance.\n Display Top20 Highest Costing Items grouped by CostCenter Detail Description and tags  SELECT \u0026#34;bill_payer_account_id\u0026#34;,\r\u0026#34;product_product_name\u0026#34;,\r\u0026#34;line_item_usage_type\u0026#34;,\r\u0026#34;line_item_line_item_description\u0026#34;,\rresource_tags_user_cost_center,\rround(sum(line_item_unblended_cost), 2) as cost\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rWHERE length(\u0026#34;resource_tags_user_cost_center\u0026#34;) \u0026gt; 0\rGROUP BY \u0026#34;resource_tags_user_cost_center\u0026#34;,\r\u0026#34;bill_payer_account_id\u0026#34;,\r\u0026#34;product_product_name\u0026#34;,\r\u0026#34;line_item_usage_type\u0026#34;,\r\u0026#34;line_item_line_item_description\u0026#34;\rORDER BY cost desc\rLIMIT 20 Query results returned  Display Top20 Highest Costing Items grouped by Detailed Description and omit the CostCenter tag  SELECT \u0026#34;bill_payer_account_id\u0026#34;,\r\u0026#34;product_product_name\u0026#34;,\r\u0026#34;line_item_usage_type\u0026#34;,\r\u0026#34;line_item_line_item_description\u0026#34;,\rround(sum(line_item_unblended_cost), 2) as cost\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rWHERE length(\u0026#34;resource_tags_user_cost_center\u0026#34;) = 0\rGROUP BY \u0026#34;bill_payer_account_id\u0026#34;,\r\u0026#34;product_product_name\u0026#34;,\r\u0026#34;line_item_usage_type\u0026#34;,\r\u0026#34;line_item_line_item_description\u0026#34;\rORDER BY cost desc\rLIMIT 20 Query results returned  "
},
{
	"uri": "/4-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "Clean up resources Remove AWS Glue Crawler  Access the AWS Glue interface Select Crawlers Choose the crawler related to the lab Select Actions and select Delete crawler Select Delete  Delete database in AWS Glue  Access the AWS Glue interface Select Databases Select the database related to the lab Select Delete  Delete S3 bucket  Access S3 interface Select Bucket Select the bucket related to the lab Select Empty  Enter permanently delete and select Empty  Select the bucket related to the lab and select Delete  Verify the bucket name and select Delete bucket  "
},
{
	"uri": "/3-costandusageanalysis/3.4-usage/",
	"title": "Usage",
	"tags": [],
	"description": "",
	"content": "Savings Plans, Reserved Instance, On Demand and Spot Usage To improve the use of costing models for businesses, here are a few examples that highlight EC2 use cases in Savings Plans or Reserved Instance styles. efficient way when compared to On Demand.\n Show accounts that use EC2 as Reserved Instance and how much they pay if charged at public rates. This is extremely useful to the organization in calculating reimbursement.  SELECT \u0026#34;bill_payer_account_id\u0026#34;,\r\u0026#34;bill_billing_period_start_date\u0026#34;,\r\u0026#34;line_item_usage_account_id\u0026#34;,\r\u0026#34;reservation_reservation_a_r_n\u0026#34;,\r\u0026#34;line_item_product_code\u0026#34;,\r\u0026#34;line_item_usage_type\u0026#34;,\rsum(\u0026#34;line_item_usage_amount\u0026#34;) as Usage,\r\u0026#34;line_item_unblended_rate\u0026#34;,\rsum(\u0026#34;line_item_unblended_cost\u0026#34;) as Cost,\r\u0026#34;line_item_line_item_description\u0026#34;,\r\u0026#34;pricing_public_on_demand_rate\u0026#34;,\rsum(\u0026#34;pricing_public_on_demand_cost\u0026#34;) as PublicCost\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rWHERE \u0026#34;line_item_line_item_Type\u0026#34; like \u0026#39;%DiscountedUsage%\u0026#39;\rGROUP BY \u0026#34;bill_payer_account_id\u0026#34;,\r\u0026#34;bill_billing_period_start_date\u0026#34;,\r\u0026#34;line_item_usage_account_id\u0026#34;,\r\u0026#34;reservation_reservation_a_r_n\u0026#34;,\r\u0026#34;line_item_product_code\u0026#34;,\r\u0026#34;line_item_usage_type\u0026#34;,\r\u0026#34;line_item_unblended_rate\u0026#34;,\r\u0026#34;line_item_line_item_description\u0026#34;,\r\u0026#34;pricing_public_on_demand_rate\u0026#34;\rLIMIT 20 Query results returned  Display cost for each usage type of family t2  SELECT \u0026#34;line_item_usage_type\u0026#34;,\rsum(\u0026#34;line_item_usage_amount\u0026#34;) as usage,\rround(sum(\u0026#34;line_item_unblended_cost\u0026#34;), 2) as cost\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rWHERE \u0026#34;line_item_usage_type\u0026#34; like \u0026#39;%t2.%\u0026#39;\rGROUP BY \u0026#34;line_item_usage_type\u0026#34;\rORDER BY \u0026#34;line_item_usage_type\u0026#34;\rLIMIT 20 Query results returned  Display hourly cost for each usage type  SELECT \u0026#34;line_item_usage_type\u0026#34;,\rround(sum(\u0026#34;line_item_usage_amount\u0026#34;), 2) as usage,\rround(sum(\u0026#34;line_item_unblended_cost\u0026#34;), 2) as cost,\rround(\ravg(\r\u0026#34;line_item_unblended_cost\u0026#34; / \u0026#34;line_item_usage_amount\u0026#34;\r),\r4\r) as hourly_rate\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rWHERE \u0026#34;line_item_product_code\u0026#34; like \u0026#39;%AmazonEC2%\u0026#39;\rand \u0026#34;line_item_usage_type\u0026#34; like \u0026#39;%Usage%\u0026#39;\rGROUP BY \u0026#34;line_item_usage_type\u0026#34;\rORDER BY \u0026#34;line_item_usage_type\u0026#34;\rLIMIT 20 Query results returned  Display Number of Reserved Instances that have not been used  SELECT bill_billing_period_start_date,\rproduct_region,\rline_item_usage_type,\rreservation_reservation_a_r_n,\rreservation_unused_quantity,\rreservation_unused_recurring_fee\rFROM \u0026#34;costmaster\u0026#34;.\u0026#34;monthly_report\u0026#34;\rWHERE length(reservation_reservation_a_r_n) \u0026gt; 0\rand reservation_unused_quantity \u0026gt; 0\rORDER BY bill_billing_period_start_date,\rreservation_unused_recurring_fee desc\rLIMIT 20 Query results returned  "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]